{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "J3NzmACPjF0g",
    "nbgrader": {
     "checksum": "e9415216345b6fd30b4cd117eddc5d31",
     "grade": false,
     "grade_id": "cell-da7e5eaae3b1d9a5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Zomato Recommendation Engine\n",
    "\n",
    "Make a restaurant recommendation engine for Zomato using user review data.\n",
    "\n",
    "![alt text](zomato-banner.png)\n",
    "\n",
    "## Prologue\n",
    "\n",
    "You have been lucky and after graduation got employed as a Data Engineer at Zomato. You have been living in Mumbai, spending your big fat check, but due to the recent emergence of Swiggy, Zomato is now facing a major competitor in the market. Your boss Dr Mukherjee, given the situation, has assigned you to improve the recommendation engine employed at Zomato. It is now upon you to keep your stocks floating and grab that year-end bonus.\n",
    "\n",
    "It has been two days since you were assigned the project, you researched about recommender systems, for which you found [this blog](https://towardsdatascience.com/brief-on-recommender-systems-b86a1068a4dd) extremely helpful. After reading the blog, you decided to make a **collaborative filtering based recommendation system using autoencoders**, since Zomato was already using content-based filtering which wasn't giving promising results. You decided to use user review data on restaurants situated in Mumbai since it has a high user engagement. Following are the notes, you made while researching:\n",
    "\n",
    "<!-- In this assignment, we will be working on implementing a recommendation system on Zomato's user reviews data for different restaurants. We will be using a technique called Collaborative Filtering which is one of the most popular ways of implementing a recommendation system and we will be training an autoencoder. Don't be afraid if you have never heard these terms before we will be explaining them in detail. -->\n",
    "\n",
    "## Collaborative Filtering\n",
    "\n",
    "Let's say we have ratings of 5 users on 4 different restaurants and a user might not have rated a particular restaurant. We can create arrange the ratings in a user-restaurant matrix from the data, that looks like this:\n",
    "\n",
    "![alt text](data.png)\n",
    "\n",
    "As you can see the users have not rated all the restaurants. In collaborative filtering we try to find these missing values and based on these values we make a recommendation to them. In this figure, for example, Kabir has rated ANC and Looters but not Food King, however Suvigya has rated all three of these restaurants. We can observe that the ratings of Suvigya on ANC and Looters match a lot with Kabir and we can to some extent say that Kabir's rating for Food King will be close to 2. \n",
    "\n",
    "## Autoencoders\n",
    "\n",
    "**Youtube Video:** https://www.youtube.com/watch?v=9zKuYvjFFS8 (till 5:36)\n",
    "\n",
    "An autoencoder is a neural network which learns to copy its input to its output. The network can be viewed as consisting 2 parts, an encoder which downsamples the input to a compact representation and a decoder which produces a reconstruction of the input. Autoencoders falls under the area of unsupervised learning algorithms, since for training an autoencoder we only need raw data without any label. An autoencoder with one hidden layer is shown below:\n",
    "\n",
    "\n",
    "![alt text](autoencoder.png) \n",
    "\n",
    "Please note that we can have as many layers as we want in our autoencoder. The only thing to keep in mind is to reduce the dimensionality of the input in the encoder part of the network and increase the dimensionality of the hidden representation to the size of actual input in decoder network.\n",
    "\n",
    "Once we have defined the architecture of our autoencoder, we can train it using the following steps:\n",
    "\n",
    "1.   Feed the batch of inputs *x* to the network and obtain the predictions *p*\n",
    "2.   Compute the loss between the reconstructed inputs *p* and the actual inputs *x* i.e. *L(p,x)*\n",
    "3. Compute the gradients of this loss with respect to the parameters of the model using backprop.\n",
    "4. Use these gradients to take optimization step, for e.g. Stochastic Gradient Descent.\n",
    "5. Repeat steps 1-4 till convergence.\n",
    "\n",
    "## Collaborative Filtering using Autoencoders\n",
    "\n",
    "We will now see how to use autoencoders for collaborative filtering. Lets first see the steps involved in training an autoencoder for this problem and then we will develope an intuition on why it works.\n",
    "\n",
    "![alt text](autoencoder2.png)\n",
    "\n",
    "In the figure we are feeding Kabir's ratings to the autoencoder. Note that the ratings corresponding to the missing values are kept zero. After computing the predictions of autoencoder we compute mean squared error between the input ratings and the ratings predicting by the network. One important thing to note here is that while computing the loss we do not consider the predictions corresponding to 0 ratings i.e. the missing values and only compute the error for the ratings that we are given. This is because we do not want our network to learn that the output corresponding to the missing values is zero but instead learn a reasonable value. We repeat the same thing for other users and then backprogate the gradients and take optimization steps.\n",
    "\n",
    "During test time we will feed the input with the missing ratings to the network and use the output of the network to determine the ratings of the missing values. Once we have the ratings for those missing values we can make a recommendation accordingly.\n",
    "\n",
    "![alt text](autoencoder3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "abUEaoSCjE-f",
    "nbgrader": {
     "checksum": "29cebb90824aa5122632b2238dd7006e",
     "grade": false,
     "grade_id": "cell-0b9e585d43a72804",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "After reading your notes, you decided to proceed with building the recommendation engine.\n",
    "\n",
    "Fortunately, you don't have to worry about the data collection as your team member Suvigya has already collected the data from Zomato's website in the required format using the code [here](https://github.com/suvigyavijay/zomato-scraper).\n",
    "\n",
    "![alt text](restaurants_data.png)\n",
    "\n",
    "![alt text](review_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "M1vv3u8nt_qi",
    "nbgrader": {
     "checksum": "c63c23b9454a8269ed6a035f486ef104",
     "grade": false,
     "grade_id": "cell-00df6203e24d157d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "After the data collection, another team member, Kabir helped out and extracted the ratings and preprocessed it. Finally, he provided you with the train-test split, in order to ensure that you spent more time on designing the algorithm instead of engineering the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "osxqQarigNPn",
    "nbgrader": {
     "checksum": "edec86f6d57439a68af85d8c2c38764e",
     "grade": false,
     "grade_id": "cell-081723df16cfa8eb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#Loading necessary packages and defining global variables\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sys import platform\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "device = torch.device(\"cpu\")\n",
    "OUTPUT_DIR_TRAIN='data/train.dat'\n",
    "OUTPUT_DIR_TEST='data/test1.dat'\n",
    "\n",
    "NUM_RESTS = 5138\n",
    "NUM_USERS = 3579"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "R4DizmM_h0Oq",
    "nbgrader": {
     "checksum": "0069b3e75ea009ec628d82ee5d768889",
     "grade": false,
     "grade_id": "cell-1cd7a8160cc7217d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Restaurant reviews data\n",
    "\n",
    "We have reviews of 5138 restaurants from 3579 different users. Each user is given an ID from 0 to 3578 and similarly, each restaurant is given an ID from 0 to 5137. We have provided you 3 files `train.dat` and `test1.dat` and `test_hidden.dat` containing training data, testing data and hidden test data respectively. For now, do not worry about `train_hidden.dat`. The format of data in the other 2 files is:\n",
    "\n",
    "                        userID, restaurantID, rating\n",
    "                        \n",
    "This means each row contains a rating given to a particular restaurant by a given user. Please have a look at the files and ensure what we said makes sense. The figure below shows how the split has been made from the original data to training and test set.\n",
    "\n",
    "![alt text](datasplit.png)\n",
    "\n",
    "We have removed some ratings from each user at random from train set and added them to test set. This way we can compare the final predictions on the train set and compare with ratings present in the test set, to find out how the network is actually performing.\n",
    "\n",
    "During training, we use the rating data from the `train.dat` file to train our autoencoder. During testing though we feed the data from `train.dat` to the autoencoder and compare the predictions with the values given in `test.dat`. The figure given below demonstrates the testing process.\n",
    "\n",
    "![alt text](testing.png)\n",
    "\n",
    "`test_hidden.dat` only contains the userID and restaurantID and not the ratings. Finally, you will be making predictions for these restaurants, user pairs and will be submitting your predictions to the Kaggle competition. We will then evaluate your predictions against the actual ratings and you will be ranked on the leaderboard accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "7PlmOgogmlnN",
    "nbgrader": {
     "checksum": "39c325d875648c1c139c9d529dcc6309",
     "grade": false,
     "grade_id": "cell-6ac0cf635ccdd4b7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 1: Implement get_sparse_mat function (2 marks)\n",
    "\n",
    "Before we start building our model, we first need to get our data in a suitable form which can be fed to the network. The input to the network will be a vector of size 5138 (number of restaurants) for each user, where each element of the vector will contain the rating of the restaurant whose id matches with the index of that element. If the user has not rated a particular restaurant, the element corresponding to that will be set to zero. \n",
    "\n",
    "For example: If a user has only rated the restaurants with ids 3, 19 and 1009 as 2, 4 and 3.5 respectively, then the feature vector for that user will be a 5138 sized array containing 2 in the index 3, 4 in the index 19, 3.5 in index 1009 and zero everywhere else.\n",
    "\n",
    "Hence our full dataset will be a matrix of size 3579 X 5138 (number of users X number of restaurants).\n",
    "\n",
    "Since we know that the users have reviewed only a small portion of all 5138 restaurants, the data matrix will be very sparse (will mostly contain zeros). Hence it does not make sense to store the whole 3579 X 5138 matrix in memory. Instead, we create a sparse matrix where for each user we store the tuples containing the restaurant id and the rating for that restaurant. \n",
    "\n",
    "For example: In the previous example where we had a user who rated restaurants 3, 19 and 1009 only, we will store the tuples [(3, 2), (19, 4), (1009, 3.5)]. Earlier we store an array of size 5138 elements for the user but now we need to have an array containing 3 elements, saving much memory. While feeding the inputs to the neural net we convert this sparse representation to the full 5138-dimensional vector.\n",
    "\n",
    "The function `get_sparse_mat` takes as the input the filename string which can either be `train.dat` and `test.dat` and constructs a sparse matrix containing the list of tuples for each user as we described above. The output of the function should be a python list of size 3579 with each element being a list of tuples.\n",
    "\n",
    "![alt text](get_sparse_mat.png)\n",
    "\n",
    "**Note 1:** You can read .dat files similar to how you read .txt files. Use python's inbuilt function *open* to create a file pointer lets say *fp*, then you can use functions like read, readline etc. to read the data values from the file. Refer to this [link](https://www.programiz.com/python-programming/file-operation) if you want a refresher to file IO in python.\n",
    "\n",
    "**Note 2:** You can also read .dat files using pandas similar to the way you read CSV files using *pd.read_csv* function.\n",
    "\n",
    "**Note 3:** The tuples in the list (restaurantID, rating) should have restaurantID as an integer value and rating as a float.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "Lxobl5WogNP-",
    "nbgrader": {
     "checksum": "4b57d84e6c6c65a139ac0a5747e95c3d",
     "grade": false,
     "grade_id": "cell-cf39d53fdc8c6aa1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_sparse_mat(filename):\n",
    "  \n",
    "    '''\n",
    "    \n",
    "    Inputs: \n",
    "        -filename: a string containing the name of the file from which we want\n",
    "                    to extract the data. In our case it can be either train.dat\n",
    "                    or test.dat\n",
    "                    \n",
    "    Returns a python list of size 3579 (number of users) with each element of\n",
    "    the list being a list of tuples (restaurantID, rating).\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    sparse_mat = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return sparse_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "I_KAvnHpgNQB",
    "nbgrader": {
     "checksum": "452258bee1a41fa3ff88ab4068bcbc9a",
     "grade": false,
     "grade_id": "cell-046803b61d6cfe97",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#Now that we have implemented the get_sparse_mat function we can get the train and test sparse matrices\n",
    "train_smat = get_sparse_mat(OUTPUT_DIR_TRAIN)\n",
    "test_smat = get_sparse_mat(OUTPUT_DIR_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7d2883aabb135bdd2d4613418e3cbce9",
     "grade": false,
     "grade_id": "cell-f2814ee9b5f9f775",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "                            \"\"\"Don't change code in this cell\"\"\"\n",
    "#SAMPLE TEST CASE\n",
    "print(\"Running Sample Test Case 1\")\n",
    "assert np.allclose(len(train_smat), 3579)\n",
    "print(\"Sample Test Case 1 Passed\")\n",
    "print(\"Running Sample Test Case 2\")\n",
    "assert np.allclose(len(test_smat), 3579)\n",
    "print(\"Sample Test Case 2 Passed\")\n",
    "print(\"Running Sample Test Case 3\")\n",
    "assert np.allclose(len(train_smat[5]), 234)\n",
    "print(\"Sample Test Case 3 Passed\")\n",
    "print(\"Running Sample Test Case 4\")\n",
    "assert np.allclose(train_smat[5][:5],[(626, 4.0), (1177, 4.5), (976, 4.0), (3926, 4.0), (3274, 5.0)])\n",
    "print(\"Sample Test Case 4 Passed\")\n",
    "print(\"Running Sample Test Case 5\")\n",
    "assert np.allclose(len(test_smat[5]),5)\n",
    "print(\"Sample Test Case 5 Passed\")\n",
    "print(\"Running Sample Test Case 6\")\n",
    "assert np.allclose(test_smat[5][:5], [(574, 3.5), (3717, 4.0), (2303, 4.0), (863, 3.5), (1706, 1.0)])\n",
    "print(\"Sample Test Case 6 Passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "530a3db80df66fb50b4c0fdd341166e1",
     "grade": true,
     "grade_id": "cell-c38d8939b302b1fb",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "                            \"\"\"Don't change code in this cell\"\"\"\n",
    "#HIDDEN TEST CASE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "54ca406695247bd0f6ff3045000d7436",
     "grade": false,
     "grade_id": "cell-d1aec5dfd6f1c04a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Dataloaders\n",
    "\n",
    "Next we have defined a dataset class which can be used to efficiently iterate through the dataset. We have provided its implementation for you. Go through it once and make sure you understand it. Using the dataset objects we define data generators for train and test sets which are used to get batches of input data. To learn about how pytorch's Dataset and Dataloader classes work in detail please go through this link: https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "lhoVHE5zgNQO",
    "nbgrader": {
     "checksum": "d5096e55eb1dc96880add179cb6d748b",
     "grade": false,
     "grade_id": "cell-69be83684ed5d6b4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X_sam = torch.zeros(5138)\n",
    "        y_sam = torch.zeros(5138)\n",
    "        for i in range(len(self.X[index])):\n",
    "            X_sam[self.X[index][i][0]] = self.X[index][i][1]\n",
    "\n",
    "        for i in range(len(self.y[index])):\n",
    "            y_sam[self.y[index][i][0]] = self.y[index][i][1]\n",
    "\n",
    "        return X_sam, y_sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "CUzq0fE9gNQQ",
    "nbgrader": {
     "checksum": "f21287a1441a259dba01f43d08dd1928",
     "grade": false,
     "grade_id": "cell-b54acc0bc04f91dc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_smat,train_smat)\n",
    "test_dataset = Dataset(train_smat, test_smat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "xCBvEhgfgNQS",
    "nbgrader": {
     "checksum": "7d71655f31dacd96bb64f9d16ef56551",
     "grade": false,
     "grade_id": "cell-56723c374d90462f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "params = {'batch_size': 64,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6 if platform == 'linux' else 0}\n",
    "training_generator = data.DataLoader(train_dataset, **params)# sampler = torch.utils.data.SequentialSampler(train_dataset))\n",
    "validation_generator = data.DataLoader(test_dataset, **params)# sampler = torch.utils.data.SequentialSampler(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b86b7dd253db3106870aa47f6e404e32",
     "grade": false,
     "grade_id": "cell-027eaa95f0e4f691",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Implementing Autoencoder Architecture\n",
    "\n",
    "Now that we have our datasets ready we can define the architecture of our autoencoder network. We typically define a network architecture in pytorch by extending the nn.Module class. In the cell below we have demonstrated how you can implement a 3 layer neural network with the following architecture in pytorch.\n",
    "\n",
    "INPUT(size = 100) -> FC+ReLU(size = 64) -> FC+ReLU(size = 32) -> FC(size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "20ce18f4199dec57a6f9aafa23fe4af2",
     "grade": false,
     "grade_id": "cell-9bab307c20e18894",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class threeLayerNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        In constructor we define different layers we will use in our architecture.\n",
    "        '''\n",
    "        #Constructor call to the superclass\n",
    "        super(threeLayerNet, self).__init__()\n",
    "        #Defining the layers to be used in the network. \n",
    "        #nn.Linear defines a fully connected layer and the first argument represents the input size and the second represents the output size.\n",
    "        self.layer1 = nn.Linear(100, 64)\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.layer3 = nn.Linear(32, 10)\n",
    "        #Defining the activation function to be used in the network\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        '''\n",
    "        The forward function takes passes the input through the layers and returns the output.\n",
    "        Inputs:\n",
    "            -x : Input tensor of shape [N_batch, 100]\n",
    "            \n",
    "        Returns the output of neural network of shape [N_batch, 10]\n",
    "        '''\n",
    "        \n",
    "        out = self.layer1(x)\n",
    "        out = self.act(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.act(out)\n",
    "        out = self.layer3(out)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "57d35c836179241f3bdff496cfdeacf1",
     "grade": false,
     "grade_id": "cell-378bee8b83d93cad",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#Once we have defined the network class we can create an instance for it.\n",
    "net = threeLayerNet()\n",
    "\n",
    "#To get the output of the network on data just call the network instance and feed the inputs\n",
    "\n",
    "x = torch.rand(5, 100) #Just a random input\n",
    "network_prediction = net(x)\n",
    "network_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c617d2b3f04cc4ff7b57454a354d5bd8",
     "grade": false,
     "grade_id": "cell-fb53aaf110fc5b33",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 2.1 Warmup Excercise: Implement a toy neural network. (1 Mark)\n",
    "\n",
    "Before you implement the autoencoder architecture as a practice implement a simple 2 layer neural with the following architecture:\n",
    "\n",
    "INPUT(size = 224) -> FC+ReLU(size = 128) -> FC+tanh(size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b0e8755de282d098ba187d27a94509b5",
     "grade": false,
     "grade_id": "cell-6d182e2cd570df03",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class twolayerNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Define the layers and activation functions to be used in the network.\n",
    "        '''\n",
    "        super(twolayerNet, self).__init__()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        '''\n",
    "        Implement the forward function which takes as input the tensor x and feeds it to the layers of the network\n",
    "        and returns the output.\n",
    "        \n",
    "        Inputs:\n",
    "            -x : Input tensor of shape [N_batch, 224]\n",
    "            \n",
    "        Returns the output of neural network of shape [N_batch, 5]\n",
    "        '''\n",
    "        \n",
    "        out = torch.zeros(x.shape[0], 5)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8c0ce61ba0bb751ced73775019b37367",
     "grade": false,
     "grade_id": "cell-cc9f5d7a2bc177bd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "net = twolayerNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "17d5b6b37895e8d6b1e3a7a7132a73ee",
     "grade": false,
     "grade_id": "cell-50e1ec89e1826837",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "                            \"\"\"Don't change code in this cell\"\"\"\n",
    "    \n",
    "### SAMPLE TEST CASE\n",
    "params_shapes = [p.shape for p in net.parameters()]\n",
    "params_shapes = sorted(params_shapes)\n",
    "print(\"Running Sample Test Case\")\n",
    "assert params_shapes ==[torch.Size([5]),\n",
    " torch.Size([5, 128]),\n",
    " torch.Size([128]),\n",
    " torch.Size([128, 224])]\n",
    "print(\"Sample Test Case Passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "897129ade8fd56a7d2d38ede753cbd67",
     "grade": true,
     "grade_id": "cell-4c078de99f5df1b2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "                            \"\"\"Don't change code in this cell\"\"\"\n",
    "    \n",
    "### HIDDEN TEST CASE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d47bf25e8ec61dca6db0d08f0b3102f9",
     "grade": false,
     "grade_id": "cell-54a588d06063e4d4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 2.2: Implement the deep autoencoder (2.5 Marks)\n",
    "\n",
    "Now you will implement the autoencoder network which we will be using to build our recommendation system. The architeture of the network should be:\n",
    "\n",
    "INPUT(size = 5138) -> FC+Tanh(size = 32) -> FC(size = 5138);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "bR2dyOJ7gNQU",
    "nbgrader": {
     "checksum": "1956ace2b07ae51d5528fc7cf9024b7d",
     "grade": false,
     "grade_id": "cell-7b08f5cd0d9e77c4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class DAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Define the layers and activation functions to be used in the network.\n",
    "        '''\n",
    "        super(DAE,self).__init__()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Implement the forward function which takes as input the tensor x and feeds it to the layers of the network\n",
    "        and returns the output.\n",
    "        \n",
    "        Inputs:\n",
    "            -x : Input tensor of shape [N_batch, 5138]\n",
    "            \n",
    "        Returns the output of neural network of shape [N_batch, 5138]\n",
    "        '''\n",
    "        \n",
    "        out = torch.zeros(x.shape[0], 5138)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "CyLsCCpKgNQX",
    "nbgrader": {
     "checksum": "169ab09281f378e7b1dcea6490e31cd6",
     "grade": false,
     "grade_id": "cell-69d680935a26da6c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "net = DAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b751d9b11363e57a3b81c5d9c96effac",
     "grade": false,
     "grade_id": "cell-ab01b51dd537bda5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "                            \"\"\"Don't change code in this cell\"\"\"\n",
    "    \n",
    "### SAMPLE TEST CASE\n",
    "params_shapes = [p.shape for p in net.parameters()]\n",
    "params_shapes = sorted(params_shapes)\n",
    "print(\"Running Sample Test Case\")\n",
    "assert params_shapes == [torch.Size([32]), torch.Size([32, 5138]), torch.Size([5138]), torch.Size([5138, 32])]\n",
    "print(\"Sample Test Case Passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2cf3bdf8a881695d7ced300342355d39",
     "grade": true,
     "grade_id": "cell-3cdd61c6cf668813",
     "locked": true,
     "points": 2.5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "                            \"\"\"Don't change code in this cell\"\"\"\n",
    "### HIDDEN TEST CASE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a414a2c131c45fb48c1a64dd64943774",
     "grade": false,
     "grade_id": "cell-646d884972dae893",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 3: Implement the loss function (2.5 Marks)\n",
    "\n",
    "Now that we have defined our autoencoder network we need to define a loss function to train our model. We will be using mean squared error as our loss function which can be simply implemented by taking the squared sum of the errors between the model predictions and the labels and dividing it by the number of training examples. However, there is a small catch here. As we described in the beginning, for a user we have to compute this error for the restaurants whose ratings have been given by the user and not for the restaurants with the missing ratings.\n",
    "\n",
    "![loss.png](loss.png)\n",
    "\n",
    "Please note that in the figure we are dividing the sum of squared errors with 4 which comes from the total number of ratings that are available in the input data.\n",
    "\n",
    "The function masked_loss takes as the input predictions and labels and calculates the mean squared error for the available ratings. One way of doing this is to first define a mask which is zero for the ratings not available and one for the available ones. Then we multiply this mask with the model predictions so that it zeros out the predictions of the network which are missing in the input data. Now we can calculate the sum of squared errors between the masked predictions and the input ratings and divide it with the number of available ratings which can be calculated by counting the number of ones in the mask.\n",
    "\n",
    "![maskedloss1.png](maskedloss1.png)\n",
    "![maskedloss2.png](maskedloss2.png)\n",
    "![maskedloss3.png](maskedloss3.png)\n",
    "\n",
    "Hint: You might find torch.where function useful in creating the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "kNtI-Z3BgNQZ",
    "nbgrader": {
     "checksum": "e7326b8c37919cdecdbaafc1cd6b4a1d",
     "grade": false,
     "grade_id": "cell-f6aaae25b10cb2eb",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def masked_loss(preds, labels):\n",
    "    \n",
    "    '''\n",
    "    Inputs:\n",
    "        -preds: Model predictions [N_batch, 5138]\n",
    "        -labels: User ratings [N_batch, 5138]\n",
    "        \n",
    "    Returns the masked loss as described above.\n",
    "    '''\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cc135435ef4774cdd562f25904f13e42",
     "grade": false,
     "grade_id": "cell-a620e81451705a29",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "                            \"\"\"Don't change code in this cell\"\"\"\n",
    "    \n",
    "### SAMPLE TEST CASE\n",
    "x = torch.zeros(3, 5138)\n",
    "x[0][100] = 1\n",
    "x[0][7] = 1\n",
    "x[0][1009] = 1\n",
    "x[1][101] = 1\n",
    "x[1][8] = 1\n",
    "x[1][1010] = 1\n",
    "x[1][56] = 1\n",
    "x[2][102] = 1\n",
    "x[2][9] = 1\n",
    "loss = masked_loss(net(x), x).item()\n",
    "print(\"Running Sample Test Case\")\n",
    "assert np.allclose(loss, 1.1857765913009644, atol = 1e-4)\n",
    "print(\"Sample Test Case Passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "40959e518967e9a6a71cef465208b47d",
     "grade": true,
     "grade_id": "cell-830157d8d2f759fa",
     "locked": true,
     "points": 2.5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "                            \"\"\"Don't change code in this cell\"\"\"\n",
    "    \n",
    "### HIDDEN TEST CASE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "987ada0ec3a405b513100e6c1efa6bba",
     "grade": false,
     "grade_id": "cell-527133cfdf013836",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Defining the Optimizer\n",
    "\n",
    "Now that we have our autoencoder architecture and loss function ready we only need to define an optimizer object before we begin training. We will be using Stochastic Gradient Descent (SGD) optimizer with a learning rate of 0.1. Pytorch comes with optim module which contains different optimizers for optimizing neural nets. We can define an instance of an optimizer as:\n",
    "optim.OptimizerName(net.parameters(), learning_rate), where the first argument corresponds to the list of parameters of the network and the second parameter being the learning rate. We have defined SGD optimizer object in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "V3kqoSF9gNQg",
    "nbgrader": {
     "checksum": "89ea1aab370049714e6d70818c701b32",
     "grade": false,
     "grade_id": "cell-2b594031f6db722f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "opti = optim.SGD(net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9a698106db05852cee7584ba6c2834ea",
     "grade": false,
     "grade_id": "cell-8caf5c05eee786bb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 4: Training the model (2 marks)\n",
    "\n",
    "Now we have everything ready to start training our model. Each iteration will consist of 4 important steps:\n",
    "\n",
    "1. Feeding input data to the network and obtaining model predictions\n",
    "2. Compute the loss between inputs and labels say *loss*\n",
    "3. Backpropagate the gradients using *loss.backward()*\n",
    "4. Take the optimization step using the *step* method of the optimizer instance.\n",
    "\n",
    "This function will be manually graded since there the training results might vary each time we run the network. Effectively you should see a decrease in both train and validation losses. The final training loss should be around 1.7 and validation loss should be around 2 to 2.2 towards the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "zYZnTbjIgNQv",
    "nbgrader": {
     "checksum": "971183cb24dcf0c0f3557918f46952a8",
     "grade": false,
     "grade_id": "cell-919c9ecf6bab12d3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "5bb2ec97-ae30-4b5c-dc4d-4c8d3db0f999"
   },
   "outputs": [],
   "source": [
    "def train(net, criterion, opti, training_generator, validation_generator, max_epochs = 10):\n",
    "    \n",
    "    '''\n",
    "    Inputs:\n",
    "        - net: The model instance\n",
    "        - criterion: Loss function, in our case it is masked_loss function.\n",
    "        - opti: Optimizer Instance\n",
    "        - training_generator: For iterating through the training set\n",
    "        - validation_generator: For iterating through the test set\n",
    "        - max_epochs: Number of training epochs. One epoch is defined as one complete presentation of the data set.\n",
    "    \n",
    "    Outputs:\n",
    "        - train_losses: a list of size max_epochs containing the average loss for each epoch of training set.\n",
    "        - val_losses: a list of size max_epochs containing the average loss for each epoch of test set.\n",
    "        \n",
    "        Note: We compute the average loss in an epoch by summing the loss at each iteration of that epoch\n",
    "        and then dividing the sum by the number of iterations in that epoch.\n",
    "    '''\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        running_loss = 0 #Accumulate the loss in each iteration of the epoch in this variable\n",
    "        cnt = 0 #Increment it each time to find the number iterations in the epoch.\n",
    "        # Training iterations\n",
    "        for batch_X, batch_y in training_generator:\n",
    "            opti.zero_grad() #Clears the gradients of all variables.\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        print(\"Epoch {}: Training Loss {}\".format(epoch+1, running_loss/cnt))\n",
    "        train_losses.append(running_loss/cnt)\n",
    "        \n",
    "        \n",
    "        #Now that we have trained the model for an epoch, we evaluate it on the test set\n",
    "        running_loss = 0\n",
    "        cnt = 0\n",
    "        with torch.set_grad_enabled(False):\n",
    "            for batch_X, batch_y in validation_generator:\n",
    "\n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "                \n",
    "        print(\"Epoch {}: Validation Loss {}\".format(epoch+1, running_loss/cnt))\n",
    "\n",
    "        val_losses.append(running_loss/cnt)\n",
    "        \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "64734d27e956c01de44fac3b4ac069ce",
     "grade": false,
     "grade_id": "cell-9a0fc7443ea41b0e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "net = DAE()\n",
    "opti = optim.SGD(net.parameters(), lr = 1e-1)\n",
    "train_losses, val_losses = train(net, masked_loss, opti, training_generator, validation_generator, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "kFWYWx9SgNQx",
    "nbgrader": {
     "checksum": "8f6039be4aa4034a85293e1578006d73",
     "grade": false,
     "grade_id": "cell-eb57fe16e29cfd8c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "4fc46a34-eb10-4d4c-8fea-552b4b57ce98"
   },
   "outputs": [],
   "source": [
    "# Finally we plot the graphs for loss vs epochs.\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.legend(['Training Loss', 'Validation Loss'])\n",
    "plt.xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "38fd6c8563bbb964934568a128e5f170",
     "grade": false,
     "grade_id": "cell-f5defeaa9ece7662",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Lets see how the network predictions compare with the actual ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "YeaZPgCMgNQ2",
    "nbgrader": {
     "checksum": "479131ddb8675ea6c7e91db52667264d",
     "grade": false,
     "grade_id": "cell-4e6ab6b3fb2269ff",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x, y = test_dataset.__getitem__(4)\n",
    "pred = net(x)\n",
    "print(\"Predicted Ratings: \", pred[y!=0].detach().numpy())\n",
    "print(\"Actual Ratings: \", y[y!=0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d18b64a0c7d7686230b97938da45fb6d",
     "grade": false,
     "grade_id": "cell-6ffa3a768c1d8f7e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 5: Improving the model (10 marks)\n",
    "\n",
    "From here onwards the assignment becomes open-ended. We have so far implemented a very basic autoencoder for this task. We obtained MSE in the range 2-2.2 on the test data which is still a very huge error and not acceptable in practice. However, there are many changes that we can make in our architecture and learning algorithm to improve its performance. For example:\n",
    "\n",
    "1. Deeper architecture, different activation functions and different hidden sizes particularly for the middle layer\n",
    "2. Using Dropout or L2 regularization to reduce overfitting\n",
    "3. Using Batch norm to improve training\n",
    "4. Different optimizers like AdaDelta, RMSProp, Adam etc etc\n",
    "5. Tuning hyperparameters like learning rate, l2 regularization constant, dropout rate etc\n",
    "6. Using a different loss function like mean absolute error.\n",
    "7. Creating model ensemble i.e. training multiple neural nets and then combining the outputs of all the networks to get the final output\n",
    "8. Different algorithm all together like Variational Autoencoders, SVD etc\n",
    "9. Train for more number of epochs, here we only trained for 20 epochs maybe more epochs can help\n",
    "10. Early Stopping, learning rate decay\n",
    "\n",
    "After you have made the changes use your final model to get predictions on all the users using the training data. We have provided you with a file `test_hidden.dat` which contains data in the form `userId, restaurantID`. The user ratings for these restaurantIDs are missing in the training set. From our model predictions we will obtain the ratings corresponding to these restaurants and store them in file `predictions.csv` in the format `userId, restaurantID, prediction`. You are then required to upload this CSV file on the kaggle competition created for this assignment along with this notebook.\n",
    "\n",
    "Below we have given a function that you can run after training your final model to generate the `predictions.csv` file. You might need to change the function a little according to the way you take predictions from your model. This function assumes that you have a pytorch model instance *net* as your final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(net, train_data = train_smat):\n",
    "    \n",
    "    def get_test_smat(filename = 'data/test_hidden.dat'):\n",
    "        sparse_dict = defaultdict(list)\n",
    "        for line in open(filename):\n",
    "            splitted_line = line.split(',')\n",
    "            sparse_dict[int(splitted_line[0])].append((int(splitted_line[1])))\n",
    "\n",
    "        sparse_mat = []\n",
    "        sKeys = sorted(sparse_dict)\n",
    "        for key in sKeys:\n",
    "            sparse_mat.append(sparse_dict[key])\n",
    "        \n",
    "        return sparse_mat\n",
    "            \n",
    "            \n",
    "    test_smat = get_test_smat()\n",
    "    preds = []\n",
    "    for i in range(len(train_data)):\n",
    "        \n",
    "        #Getting the actual vector from the sparse representation\n",
    "        x = torch.zeros(5138)\n",
    "        for j in range(len(train_data[i])):\n",
    "            x[train_data[i][j][0]] = train_data[i][j][1]\n",
    "        with torch.set_grad_enabled(False):\n",
    "            pred = net(x).detach().numpy() ## This logic might be different for your model, change this accordingly\n",
    "        \n",
    "        pred = pred[test_smat[i]]\n",
    "        user_rest_pred = np.concatenate([i*np.ones((len(pred),1),dtype=np.int),np.array(test_smat[i],dtype=np.int)[:,None], np.array(pred)[:,None]],axis = 1)\n",
    "        preds += user_rest_pred.tolist()\n",
    "        \n",
    "    preds = np.array(preds)\n",
    "    df = pd.DataFrame(preds)\n",
    "    df[0] = df[0].astype('int')\n",
    "    df[1] = df[1].astype('int')\n",
    "    df[2] = df[2].astype('float16')\n",
    "    df = df.drop(df.columns[[0, 1]], axis=1)\n",
    "    df.to_csv('predictions.csv', index = False, header = False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_predictions(net)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predictions.csv` needs to be uploaded on the Kaggle, instructions for which will be communicated through mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
